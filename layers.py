# -*- coding: utf-8 -*-
"""Untitled24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VTOTl2wIDtpohAiJC9-Vp365fWdRTNwB
"""

class PositionalEmbedding(Layer):
    def __init__(self, input_dim, output_dim, max_len=LAG, **kwargs):
        super().__init__(**kwargs)
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.max_len = max_len

        self.token_embedding = Embedding(
            input_dim=input_dim,
            output_dim=output_dim
        )

        self.position_embedding = Embedding(
            input_dim=max_len,
            output_dim=output_dim
        )

    def call(self, x):
        seq_len = tf.shape(x)[1]
        positions = tf.range(start=0, limit=seq_len, delta=1)
        positions = self.position_embedding(positions)

        x = self.token_embedding(x)
        return x + positions

class MultiheadAttention(layers.Layer):
    def __init__(self, num_heads, ff_dim=None):
        super().__init__()
        self.num_heads = num_heads
        self.ff_dim = ff_dim

        self.att = None
        self.ffn = None
        self.ln1 = layers.LayerNormalization()
        self.ln2 = layers.LayerNormalization()

    def build(self, input_shape):
        embed_dim = input_shape[-1]

        if self.ff_dim is None:
            self.ff_dim = embed_dim * 4

        self.att = layers.MultiHeadAttention(
            num_heads=self.num_heads,
            key_dim=embed_dim // self.num_heads
        )

        self.ffn = tf.keras.Sequential([
            layers.Dense(self.ff_dim, activation="relu"),
            layers.Dense(embed_dim)
        ])

    def call(self, x):
        attn_out = self.att(x, x, use_causal_mask=True)
        x = self.ln1(x + attn_out)

        ffn_out = self.ffn(x)
        x = self.ln2(x + ffn_out)

        return x[:, -1, :]